{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ0hbD7Mcwx+R51xP7sB9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hunter-z-hunter/hunter-z-hunter/blob/main/hunter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Hunter Z Hunter! \n",
        "\n",
        "[Hunter z Hunter](https://github.com/hunter-z-hunter) is a scavenger hunt game that anyone can use to automatically reward successful hunters with ether. We use a lightweight machine learning model to determine if the image submitted matches the target image of the treasure. We use [ezkl](https://github.com/zkonduit/ezkl) to generate a zero knowledge circuit of this model so that it can remain private and run on-chain for automatic crypto payments.\n",
        "\n",
        "Below is our lightweight neural net."
      ],
      "metadata": {
        "id": "UEd2qaaHmqyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we use Pytorch and Numpy to define a neural network called \"Hunt.\" Hunt is used to "
      ],
      "metadata": {
        "id": "x9SzcDw-vA0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnUBa_i5HyUo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "class Hunt(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Hunt, self).__init__()\n",
        "        # self.weight = nn.Parameter(torch.randn(28 * 28))\n",
        "        # linear, relu, linear multilayer perception\n",
        "        # conv2d relu, [(conv2d, relu), ...], linear\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        x = x.view(-1, 28*28)\n",
        "        target = target.view(-1, 28*28)\n",
        "        return torch.sqrt(torch.sum((x - target) ** 2, dim=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ezkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsllEresA347",
        "outputId": "cb5ea9c8-814f-4b14-8e79-6addc2f372c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ezkl\n",
            "  Downloading ezkl-0.0.2-py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: ezkl\n",
            "Successfully installed ezkl-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ezkl"
      ],
      "metadata": {
        "id": "3JYRvnTzAsOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a more complex "
      ],
      "metadata": {
        "id": "-XnM6C66-7Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from ezkl import export\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=5, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=5, stride=2)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.d1 = nn.Linear(48, 48)\n",
        "        self.d2 = nn.Linear(48, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 32x1x28x28 => 32x32x26x26\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # flatten => 32 x (32*26*26)\n",
        "        x = x.flatten(start_dim = 1)\n",
        "    #    x = x.flatten()\n",
        "\n",
        "        # 32 x (32*26*26) => 32x128\n",
        "        x = self.d1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # logits => 32x10\n",
        "        logits = self.d2(x)\n",
        "       \n",
        "        return logits\n",
        "\n",
        "circuit = MyModel()\n",
        "export(circuit, input_shape = [1,28,28])\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "F6IVVqJ8AYyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3L Relu"
      ],
      "metadata": {
        "id": "Ndm-kBdp9Hqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from ezkl import export\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,4, kernel_size=5, stride=2)\n",
        "        self.conv2 = nn.Conv2d(4,4, kernel_size=5, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc = nn.Linear(4*4*4, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1,1,28,28)\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = x.view(-1,4*4*4)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "circuit3L = MyModel()\n",
        "export(circuit3L, input_shape = [1,28,28])"
      ],
      "metadata": {
        "id": "LfpN_xTk9HG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create an instance of our model, pass in some random data (soon to be image vectors), and determine the Euclidian distance between them."
      ],
      "metadata": {
        "id": "sH9bAcc26cZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Hunt()\n",
        "input_image = torch.randn(1, 1, 28, 28)\n",
        "target_image = torch.randn(1, 1, 28, 28)\n",
        "distance = model(input_image, target_image)\n",
        "print(distance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cl5xe9bvFCG",
        "outputId": "0e80bf0a-3f2e-4461-bf10-907353496926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([37.8969])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "input_image = torch.randn(1, 1, 28, 28)\n",
        "fourrelu = circuit(input_image)\n",
        "print(fourrelu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQfyATR7BBwQ",
        "outputId": "43409c27-452b-472c-8c30-c702da0c5b42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0016,  0.1354, -0.0430, -0.1704,  0.0173,  0.0581, -0.1116,  0.0928,\n",
            "          0.1473, -0.0139]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "voMhQVyLyvDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "keiaMSUG-RkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After importing the JSON python file, we use the export function from ezkl to create a network.onnx file and an input.json file. These files are what ezkl inputs to generate a verifier for a model. "
      ],
      "metadata": {
        "id": "LimnSjeJ6ny1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export():\n",
        "    torch_model = model\n",
        "    # Input to the model\n",
        "    preimageShape = [3]\n",
        "    targetShape = [3]\n",
        "    x = 0.1*torch.rand(1,*preimageShape, requires_grad=True)\n",
        "    y = 0.1*torch.rand(1,*targetShape, requires_grad=True)\n",
        "    torch_out = torch_model(x, y)\n",
        "    # Export the model\n",
        "    torch.onnx.export(torch_model,               # model being run\n",
        "                      (x,y),                   # model input (or a tuple for multiple inputs)\n",
        "                      \"network.onnx\",            # where to save the model (can be a file or file-like object)\n",
        "                      export_params=True,        # store the trained parameter weights inside the model file\n",
        "                      opset_version=10,          # the ONNX version to export the model to\n",
        "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                      input_names = ['input'],   # the model's input names\n",
        "                      output_names = ['output'], # the model's output names\n",
        "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                    'output' : {0 : 'batch_size'}})\n",
        "\n",
        "    d = ((x).detach().numpy()).reshape([-1]).tolist()\n",
        "    dy = ((y).detach().numpy()).reshape([-1]).tolist()\n",
        "\n",
        "\n",
        "    data = dict(input_shapes = [preimageShape, targetShape],\n",
        "                input_data = [d, dy],\n",
        "                output_data = [((o).detach().numpy()).reshape([-1]).tolist() for o in torch_out])\n",
        "\n",
        "    # Serialize data into file:\n",
        "    json.dump( data, open( \"input.json\", 'w' ) )"
      ],
      "metadata": {
        "id": "HRH74d7Tyv4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export4():\n",
        "    torch_model = circuit\n",
        "    # Input to the model\n",
        "    preimageShape = [1, 28, 28]\n",
        "    x = 0.1*torch.rand(1,*preimageShape, requires_grad=True)\n",
        "    torch_out = torch_model(x)\n",
        "    # Export the model\n",
        "    torch.onnx.export(torch_model,               # model being run\n",
        "                      (x),                   # model input (or a tuple for multiple inputs)\n",
        "                      \"network4.onnx\",            # where to save the model (can be a file or file-like object)\n",
        "                      export_params=True,        # store the trained parameter weights inside the model file\n",
        "                      opset_version=10,          # the ONNX version to export the model to\n",
        "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                      input_names = ['input'],   # the model's input names\n",
        "                      output_names = ['output'], # the model's output names\n",
        "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                    'output' : {0 : 'batch_size'}})\n",
        "\n",
        "    d = ((x).detach().numpy()).reshape([-1]).tolist()\n",
        "\n",
        "\n",
        "    data = dict(input_shapes = [preimageShape],\n",
        "                input_data = [d],\n",
        "                output_data = [((o).detach().numpy()).reshape([-1]).tolist() for o in torch_out])\n",
        "\n",
        "    # Serialize data into file:\n",
        "    json.dump( data, open( \"input4.json\", 'w' ) )"
      ],
      "metadata": {
        "id": "lT95-TCFCY4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export3L():\n",
        "    torch_model = circuit3L\n",
        "    # Input to the model\n",
        "    preimageShape = [1, 28, 28]\n",
        "    x = 0.1*torch.rand(1,*preimageShape, requires_grad=True)\n",
        "    torch_out = torch_model(x)\n",
        "    # Export the model\n",
        "    torch.onnx.export(torch_model,               # model being run\n",
        "                      (x),                   # model input (or a tuple for multiple inputs)\n",
        "                      \"network3.onnx\",            # where to save the model (can be a file or file-like object)\n",
        "                      export_params=True,        # store the trained parameter weights inside the model file\n",
        "                      opset_version=10,          # the ONNX version to export the model to\n",
        "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                      input_names = ['input'],   # the model's input names\n",
        "                      output_names = ['output'], # the model's output names\n",
        "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                                    'output' : {0 : 'batch_size'}})\n",
        "\n",
        "    d = ((x).detach().numpy()).reshape([-1]).tolist()\n",
        "\n",
        "\n",
        "    data = dict(input_shapes = [preimageShape],\n",
        "                input_data = [d],\n",
        "                output_data = [((o).detach().numpy()).reshape([-1]).tolist() for o in torch_out])\n",
        "\n",
        "    # Serialize data into file:\n",
        "    json.dump( data, open( \"input3.json\", 'w' ) )"
      ],
      "metadata": {
        "id": "eabyATy29Y7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call export and will be using the ONNX file for our project!"
      ],
      "metadata": {
        "id": "j77RdXJw623l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "oHD_0f8pyxFw",
        "outputId": "dc0ff957-b98c-4420-ad38-e14c236e6636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d424551cb06e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: export() missing 1 required positional argument: 'torch_model'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export4()"
      ],
      "metadata": {
        "id": "xP3JS62rC9Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export3L()"
      ],
      "metadata": {
        "id": "EGiCN68T-JSx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}